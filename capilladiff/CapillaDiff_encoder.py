from torch.nn import functional as F
import pandas as pd
import numpy as np
import torch

def make_condition_ranking(condition_value, normalized: bool = True):
    """Based on the severity of the condition, rank the condition from Level 1 to Level 3.
    Args:
        condition_value: input condition_value
        normalized (bool): whether to return normalized ranking (0-1) or level (0-3)

    Returns:
        float or int: ranking of the condition
    """

    level_1 = ('+', 0.3, 1)
    level_2 = ('++', 0.6, 2)
    level_3 = ('+++', 0.9, 3)
    true_level = (0.9, 1)
    false_level = (0.1, 0)

    # 1. step check if value contains positive indicators, if True return 1
    positive_indicators = ['1', 'true', 'yes', 'positive']

    # 2. step check for severity levels
    # 3. step if none of the above, return 0

    if isinstance(condition_value, str):
        # If any positive indicator is present in the string
        for indicator in positive_indicators:
            if indicator in condition_value.lower():
                return true_level[0] if normalized else true_level[1]
        if level_3[0] in condition_value:
            return level_3[1] if normalized else level_3[2]
        elif level_2[0] in condition_value:
            return level_2[1] if normalized else level_2[2]
        elif level_1[0] in condition_value:
            return level_1[1] if normalized else level_1[2]
        return false_level[0] if normalized else false_level[1]

    elif isinstance(condition_value, (int, float)):
        return true_level[0] if normalized else true_level[1] if condition_value != 0 else false_level[0] if normalized else false_level[1]
    else:
        return false_level[0] if normalized else false_level[1]

def make_boolean(value):
    """Convert value to boolean 0/1.

    Args:
        value: input value

    Returns:
        int: 1 if value indicates True, else 0
    """
    positive_indicators = ['1', 'true', 'yes', '+', 'positive']

    if isinstance(value, str):
        # If any positive indicator is present in the string
        for indicator in positive_indicators:
            if indicator in value.lower():
                return 1
        return 0
    elif isinstance(value, (int, float)):
        return 1 if value != 0 else 0
    else:
        return 0

##### UTILS FOR CONDITION ENCODER #####

def standardize_embedding_dimension(embedding):
    """Standardize embedding dimension to (bs, 77, 768).

    Args:
        embedding (tensor): gene embedding generated by scGPT in
        512 dimensions

    Returns:
        padded_tensor (tensor): embedding padded with value 1 to have
        dimension (77, 768)"""
    # pad embedding with value 1 to have dimension 768
    # (768 is SD prompt encoding size)
    final_size = 768

    # Calculate the amount of padding on each side
    padding_left = (final_size - len(embedding)) // 2
    padding_right = final_size - len(embedding) - padding_left
    # Pad the tensor
    padded_tensor = F.pad(
        embedding, (padding_left, padding_right), 'constant', 1)
    # replicate padded_tensor to have dimension (bs, 77, 768)
    padded_tensor = padded_tensor.repeat(1, 77, 1).float()

    return padded_tensor

def get_boolean_embedding(identifier):
        """Get boolean embedding based on input condition id.

        Args:
            identifier (dict): condition identifier

        Returns:
            embedding (tensor): condition embedding
        """

        encoding = None
        true_tensor = torch.ones((1, 768)) * 0.9
        false_tensor = torch.ones((1, 768)) * 0.1
        for value in identifier.values():
            if make_boolean(value) == 1:
                # Add a layer of one-hot encoding torch.ones
                if encoding is None: encoding = true_tensor
                else: encoding = torch.cat((encoding, true_tensor), dim=0)
            else:
                if encoding is None: encoding = false_tensor
                else: encoding = torch.cat((encoding, false_tensor), dim=0)

        encoding = encoding.unsqueeze(0) # add batch dimension

        # fill up the rest of the embedding with value 0 to get shape (1, 77, 768)
        if encoding is None: encoding = torch.zeros((1, 77, 768))
        else: encoding = F.pad(encoding, (0, 0, 0, 77 - encoding.shape[1]), "constant", 0)

        return encoding

def get_level_embedding(identifier):
        """Get more complex embedding based on input condition id.
        Based on the severity of the condition, rank the condition from 0-1.

        Args:
            identifier (dict): condition identifier

        Returns:
            embedding (tensor): condition embedding
        """

        encoding = None
        true_tensor = torch.ones((1, 768)) * 0.9
        false_tensor = torch.ones((1, 768)) * 0.1
        for value in identifier.values():
            value_encoding = torch.ones((1, 768)) * make_condition_ranking(value)
            if encoding is None: encoding = value_encoding
            else: encoding = torch.cat((encoding, value_encoding), dim=0)

        encoding = encoding.unsqueeze(0) # add batch dimension

        # fill up the rest of the embedding with value 0 to get shape (1, 77, 768)
        if encoding is None: encoding = torch.zeros((1, 77, 768))
        else: encoding = F.pad(encoding, (0, 0, 0, 77 - encoding.shape[1]), "constant", 0)

        return encoding

def create_bool_text_condition(identifier):
    """Create boolean text condition based on identifier dict.
    Args:
        identifier (dict): condition identifier
    Returns:
        condition_text (str): boolean text condition
    """

    condition_parts = []
    for key, value in identifier.items():

        # convert value to boolean
        value = make_boolean(value)
        if value:
            condition_diagnosis = "True"
        else:
            condition_diagnosis = "False"

        condition_parts.append(f"{key} is {condition_diagnosis}")

    condition_text = "; ".join(condition_parts)

    return condition_text

def create_level_text_condition(identifier):
    """Create level text condition based on identifier dict.
    Args:
        identifier (dict): condition identifier
    Returns:
        condition_text (str): level text condition
    """

    condition_parts = []
    for key, value in identifier.items():

        ranking = make_condition_ranking(value, normalized=False)

        if ranking == 3:
            condition_level = "severe"
        elif ranking == 2:
            condition_level = "moderate"
        elif ranking == 1:
            condition_level = "mild"
        else:
            condition_level = "absent"

        condition_parts.append(f"{key} is {condition_level}")

    condition_text = "; ".join(condition_parts)

    return condition_text

def get_condition_embedding(identifier, model_type, convert_to_boolean, text_mode=None, clip=None):
    """Get embedding for input condition.

    Args:
        identifier (str): condition identifier
        model_type (str): model type ('conditional' or 'naive')
        convert_to_boolean (bool): whether to convert conditions to boolean embeddings
        text_mode (str or None): if 'simple', use simple text condition encoding
        clip (tuple or None): if text_mode is not None, provide (clip_tokenizer, clip_text_encoder, device)

    Returns:
        embedding (tensor): condition embedding
    """
    embedding = None
    if model_type == 'conditional':

        if text_mode is None:
            if convert_to_boolean:
                embedding = get_boolean_embedding(identifier)
            else:
                embedding = get_level_embedding(identifier)
        else:
            if text_mode == 'simple':
                if convert_to_boolean:
                    condition_text = create_bool_text_condition(identifier)
                else:
                    condition_text = create_level_text_condition(identifier)
            else:
                raise NotImplementedError("Text mode --{}-- is not implemented yet.".format(text_mode))

            clip_tokenizer, clip_text_encoder, device = clip

            inputs = clip_tokenizer(
                condition_text,
                padding="max_length",
                truncation=True,
                max_length=77,          # enforce correct shape for SD (bs, 77, 768)
                return_tensors="pt"
            ).to(device)

            with torch.no_grad():
                outputs = clip_text_encoder(**inputs)

            embedding = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]


    elif model_type == 'naive':
        embedding = torch.ones((1, 77, 768))

    else:
        raise Exception("Model type not recognized.")

    assert embedding.shape == (1, 77, 768)

    return embedding


class ConditionEncoder:

    def __init__(self, model_type: str='conditional', convert_to_boolean: bool = True, text_mode: str=None, clip=None):
        self.model_type = model_type
        self.convert_to_boolean = convert_to_boolean
        self.text_mode = text_mode
        self.clip = clip

        if not (self.model_type == 'conditional' or self.model_type == 'naive'):
            raise Exception("Model type not recognized.")
        
    def get_condition_embedding(self, identifier):
        """Return embedding based on input condition id.

        Args:
            identifier (str): condition identifier

        Returns:
            embedding (tensor): embedding
        """
        return get_condition_embedding(
            identifier, self.model_type, self.convert_to_boolean, self.text_mode, clip=self.clip)

class ConditionEncoderInference:

    def __init__(self, model_type, convert_to_boolean: bool = True, text_mode: str = None, clip=None):
        self.model_type = model_type
        self.convert_to_boolean = convert_to_boolean
        self.text_mode = text_mode
        self.clip = clip

    def __call__(self, identifier):
        """Get gene embedding for input condition.

        Args:
            identifier (str): condition identifier

        Returns:
            embedding (tensor): condition embedding
        """
        return get_condition_embedding(
            identifier, self.model_type, self.convert_to_boolean, self.text_mode, clip=self.clip)

    def get_embedding(self, identifier):
        """Return embedding based on input condition.

        Args:
            identifier (str): condition identifier

        Returns:
            embedding (tensor): embedding
        """
        return get_condition_embedding(
            identifier, self.model_type, self.convert_to_boolean, self.text_mode, clip=self.clip)

    def get_condition(self, identifier):
        """Return condition based on input condition.

        Args:
            identifier (str): condition identifier

        Returns:
            str: condition
        """
        if self.convert_to_boolean:
            return make_boolean(identifier)
        else:
            return make_condition_ranking(identifier, normalized=False)

    def get_text_mode(self):
        """Return text mode of the encoder."""
        return self.text_mode

class DictToListEncoder:
    """
    Stateless dictionary <-> string encoder.
    Encodes as:   key=value;key=value;...
    """

    def __init__(self, use_sorted_keys=False):
        self.use_sorted_keys = use_sorted_keys

    def encode(self, cond_dict):
        """
        Encode a dictionary into a single string.

        Format:
            key=value;key=value;...

        Returns:
            str
        """
        keys = sorted(cond_dict.keys()) if self.use_sorted_keys else cond_dict.keys()

        parts = []
        for k in keys:
            v = cond_dict[k]
            parts.append(f"{k}={v}")

        return ";".join(parts)

    def decode(self, encoded_string):
        """
        Decode the single encoded string back into a dictionary.

        Args:
            encoded_string (str)

        Returns:
            dict
        """
        items = encoded_string.split(";")
        out = {}

        for item in items:
            if "=" not in item:
                continue
            key, value = item.split("=", 1)

            # convert ints if possible
            if value.isdigit():
                value = int(value)

            out[key] = value

        return out